<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Automated-Testing on thingsithinkithink</title>
    <link>http://localhost:1313/tags/automated-testing/</link>
    <description>Recent content in Automated-Testing on thingsithinkithink</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 06 Jul 2025 08:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/automated-testing/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>LLM Evals Course Lesson 3: Building Automated Evaluators</title>
      <link>http://localhost:1313/posts/2025/07-06-llm-evals-course-lesson-3-automated-evaluators/</link>
      <pubDate>Sun, 06 Jul 2025 08:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/2025/07-06-llm-evals-course-lesson-3-automated-evaluators/</guid>
      <description>&lt;p&gt;The third lesson of Hamel Husain and Shreya Shankar&amp;rsquo;s &lt;a href=&#34;http://localhost:1313/posts/2025/06-08-llm-evals-lesson-1/&#34;&gt;LLM evaluation course&lt;/a&gt; moved from manual error analysis into automated evaluation systems. This session focused on the &amp;ldquo;Measure&amp;rdquo; phase of their evaluation lifecycle - how to build evaluators that can automatically detect the failure modes we identified through error analysis.&lt;/p&gt;&#xA;&lt;p&gt;In terms of the Three Gulfs Model from lesson one, this lesson first helps us distinguish between specification failures (where we need to improve our prompts) and generalisation failures (where the LLM struggles despite clear instructions). The automated evaluators we build specifically target the Gulf of Generalisation, measuring how well our LLM applies instructions across diverse inputs.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
