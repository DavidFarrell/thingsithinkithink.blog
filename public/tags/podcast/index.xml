<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Podcast on thingsithinkithink</title>
    <link>http://localhost:65142/tags/podcast/</link>
    <description>Recent content in Podcast on thingsithinkithink</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 14 Jan 2025 12:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:65142/tags/podcast/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Jackie Bavaro on Strategy</title>
      <link>http://localhost:65142/posts/2025/01-14-three-parts-of-strategy-with-jackie-bavaro/</link>
      <pubDate>Tue, 14 Jan 2025 12:00:00 +0000</pubDate>
      <guid>http://localhost:65142/posts/2025/01-14-three-parts-of-strategy-with-jackie-bavaro/</guid>
      <description>&lt;p&gt;Jackie Bavaro’s breakdown of strategy in her &lt;a href=&#34;https://www.lennysnewsletter.com/p/jackie-bavaro-on-how-to-build-product&#34;&gt;recent podcast interview&lt;/a&gt; really stuck with me. She shared a simple (but I think, pretty good) framework that highlights three key components of strategy: &lt;strong&gt;Vision&lt;/strong&gt;, &lt;strong&gt;Strategic Framework&lt;/strong&gt;, and &lt;strong&gt;Roadmap&lt;/strong&gt;.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;1-vision-this-is-where-we-want-to-go&#34;&gt;1. Vision: “This is where we want to go”&lt;/h2&gt;&#xA;&lt;p&gt;The vision is an inspiring picture of what the future looks like. Jackie framed it:&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;&lt;em&gt;“Hey everyone, this is where we want to get to—don’t you want to build this future with me?”&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>The challenges of mastering LLMs, and their role as cyborg enhancement</title>
      <link>http://localhost:65142/posts/2025/01-08-a-good-interview-with-simon-willison-on-around-the-prompt/</link>
      <pubDate>Wed, 08 Jan 2025 12:00:00 +0000</pubDate>
      <guid>http://localhost:65142/posts/2025/01-08-a-good-interview-with-simon-willison-on-around-the-prompt/</guid>
      <description>&lt;p&gt;I recently watched a good &lt;a href=&#34;https://www.youtube.com/watch?v=rLcKbvmegag&#34;&gt;interview&lt;/a&gt; with Simon Willison on Logan Kilpatrick&amp;rsquo;s podcast, hosted for Google. Simon is known for building &lt;a href=&#34;https://www.djangoproject.com/&#34;&gt;Django&lt;/a&gt; (the Python web framework), &lt;a href=&#34;https://datasette.io/&#34;&gt;Datasette&lt;/a&gt; (a data tool for journalists), and the &lt;a href=&#34;https://github.com/simonw/llm&#34;&gt;LLM command line tool&lt;/a&gt; (which lets you run large language models from the command prompt). I used LLM for months until recently &lt;a href=&#34;https://www.answer.ai/posts/2024-12-05-introducing-shell-sage.html&#34;&gt;Shell Sage&lt;/a&gt; became my daily tool.&lt;/p&gt;&#xA;&lt;h2 id=&#34;takeaways&#34;&gt;takeaways:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Simon&amp;rsquo;s take that generative AI is best thought of today as a cyborg enhancement to our capabilities. I feel this way. These things make me so impactful and able to do what I already do, but better.&#xA;&lt;ul&gt;&#xA;&lt;li&gt;However, he noted that these technologies don&amp;rsquo;t benefit everyone universally and threaten some  livelihoods.&#xA;&lt;ul&gt;&#xA;&lt;li&gt;For instance, illustrators who rely on commissions may find their work threatened as individuals opt for generating assets using tools like Stable Diffusion or Flux.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Simon&amp;rsquo;s take that LLMs are hard to master. Yes, most people have had an amazing experience that shows the power of LLMs, but it&amp;rsquo;s hard to get consistent high value results. And to do so, requires building a lot of intuition:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;The notion that a powerful computer program, such as a large language model, cannot perform basic tasks like multiplication seems absurd to many.&lt;/li&gt;&#xA;&lt;li&gt;People have to build a mental model of LLMs, understanding their strengths and weaknesses, and recognising concepts like training cut-offs, hallucinations, and the probabilistic nature of outputs.&lt;/li&gt;&#xA;&lt;li&gt;One he didn&amp;rsquo;t mention but that I&amp;rsquo;m thinking about these days is around RAG systems. Users need to understand that even when LLMs are provided with legitimate sources, those sources compete with the model&amp;rsquo;s training data, which can introduce bias against the sources.  The idea is totally foreign to most people. RAG doesn&amp;rsquo;t &lt;em&gt;fix&lt;/em&gt; anything - it has value - but it doesn&amp;rsquo;t stop the LLM saying something incorrect.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Some people think there is no longer any value to a computer science degree since LLMs can code.&#xA;&lt;ul&gt;&#xA;&lt;li&gt;But there is much more to creating great software than merely inputting a prompt and receiving functional code.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Legitimate uses of uncensored LLMs.&#xA;&lt;ul&gt;&#xA;&lt;li&gt;He shared an example of using Claude to extract campaign finance data from a report, where the AI declined to assist due to concerns about sharing personal financial information. This is absurd.&lt;/li&gt;&#xA;&lt;li&gt;In another instance, a group analysing police body cam footage had to convince the LLM that dogs can bite people, as the model was biased towards pro-dog information based on its training data (the internet!).&#xA;&lt;ul&gt;&#xA;&lt;li&gt;and yet foundation model providers are understandably hesitant to share unfiltered models due to the (extremely high) likelihood that a news institution would (of course) publish scare articles about the LLM sharing unethical content.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;thingsithinkithink&#34;&gt;thingsithinkithink&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;The thing about AI being a threat to many:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;I feel this.  I&amp;rsquo;ve worked a lot with artists, musicians, and writers.  I &lt;em&gt;know&lt;/em&gt; many of their clients would be perfectly happy with some AI &amp;lsquo;filler&amp;rsquo; assets and a lot of entry level positions in particular are at risk in these fields.&lt;/li&gt;&#xA;&lt;li&gt;I hope people in these fields are able to use AI to amplify &lt;em&gt;their&lt;/em&gt; abilities to be more impactful too - but it&amp;rsquo;s definitely going to hurt folk.  It makes me very self-conscious whenever I get excited about these things.  It&amp;rsquo;s easy to overlook the negative impacts.&lt;/li&gt;&#xA;&lt;li&gt;it does seem particularly unjust that rich silicon valley types train these tools on the work of the relatively underpaid creatives and the result is a massive threat to those already underempowered fields&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;The thing about LLMs being hard to master:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;I couldn&amp;rsquo;t agree more.  I think there is a parallel between the examples he shares about individuals having challenges to master GenAI and the organisational challenges I&amp;rsquo;ve seen over the last couple of years.&#xA;&lt;ul&gt;&#xA;&lt;li&gt;The initial wave of generative AI adoption has produced impressive proof of concepts that often fail to evolve into viable products. This challenge arises partly from the technology stack (POCs are often built on independent systems that lack integration with organisational data pipelines etc).&lt;/li&gt;&#xA;&lt;li&gt;additionally though, a significant part of the issue specifically pertains to the challenge of mastering these tools.  A POC shows what&amp;rsquo;s possible - but bringing a product to market that brings that value without causing a disaster in a multitude of edge cases is a much harder challenge.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>How AI is making its way into Government</title>
      <link>http://localhost:65142/posts/2025/01-06-podcast-interview-with-prof-alan-brown-about-uses-of-ai-in-government/</link>
      <pubDate>Mon, 06 Jan 2025 12:00:00 +0000</pubDate>
      <guid>http://localhost:65142/posts/2025/01-06-podcast-interview-with-prof-alan-brown-about-uses-of-ai-in-government/</guid>
      <description>&lt;p&gt;I&amp;rsquo;ve been thinking recently about how to frame the ways that (generative) AI is going to make its way into enterprise and the public sector.  A &lt;a href=&#34;https://www.youtube.com/watch?v=KOyovrkEhlk&amp;amp;list=PLWovoZxDVoYMQJzx9zpNjeJ3UNGuCwYLt&amp;amp;index=6&#34;&gt;recent podcast interview&lt;/a&gt; between &lt;a href=&#34;https://www.alanbrown.net/&#34;&gt;&lt;strong&gt;Professor Alan Brown&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;https://thinktankproduction.co.uk/&#34;&gt;&lt;strong&gt;Think Tank&lt;/strong&gt;&lt;/a&gt; touched upon this.&lt;/p&gt;&#xA;&lt;p&gt;Alan spent nearly two decades in the USA driving large-scale software programmes and leading R&amp;amp;D teams. After roles at Carnegie Mellon University and as an IBM Distinguished Engineer, he&amp;rsquo;s now focused on digital transformation. In addition to his research at &lt;a href=&#34;https://experts.exeter.ac.uk/27811-alan-brown&#34;&gt;Exeter&lt;/a&gt;, he&amp;rsquo;s recently published &lt;a href=&#34;https://surviveaibook.com/&#34;&gt;&lt;em&gt;&amp;ldquo;Surviving and Thriving in the Age of AI&amp;rdquo;&lt;/em&gt;&lt;/a&gt; which aims to help digital leaders navigate the challenges and opportunities of AI.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Open Models in 2024: Progress and Challenges</title>
      <link>http://localhost:65142/posts/2024/12-30-open-models-2024-progress-and-challenges/</link>
      <pubDate>Mon, 30 Dec 2024 12:00:00 +0000</pubDate>
      <guid>http://localhost:65142/posts/2024/12-30-open-models-2024-progress-and-challenges/</guid>
      <description>&lt;p&gt;I recently watched a talk from &lt;a href=&#34;https://www.youtube.com/watch?v=jX1nuoTs2WU&#34;&gt;LatentSpace Live (NeurIPS 2024)&lt;/a&gt; by &lt;a href=&#34;https://soldaini.net/&#34;&gt;Luca Soldaini&lt;/a&gt; a research scientist at the AI Institute, on the state of open models in 2024. His reflections on how far we&amp;rsquo;ve come—and the challenges ahead—offer a good perspective on where (actually-open)AI might be heading.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;the-leap-forward&#34;&gt;The Leap Forward&lt;/h2&gt;&#xA;&lt;p&gt;2024 marked a significant shift in open model capabilities. Compared to 2023 (which introduced models like Llama 1 and 2, Mistral, and Falcon), 2024 brought a new class of open models that rival frontier-level closed models. Releases like Qwen, Deep Seek, and Llama 3 show how much ground open-source AI has gained in a relatively short time.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
