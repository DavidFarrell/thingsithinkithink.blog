<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Course-Notes on thingsithinkithink</title><link>https://thingsithinkithink.blog/categories/course-notes/</link><description>Recent content in Course-Notes on thingsithinkithink</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Sun, 21 Dec 2025 09:00:00 +0000</lastBuildDate><atom:link href="https://thingsithinkithink.blog/categories/course-notes/index.xml" rel="self" type="application/rss+xml"/><item><title>LLM Evals Lesson 8: Improving LLM Products</title><link>https://thingsithinkithink.blog/posts/2025/12-21-llm-evals-lesson-8-improving-llm-products/</link><pubDate>Sun, 21 Dec 2025 09:00:00 +0000</pubDate><guid>https://thingsithinkithink.blog/posts/2025/12-21-llm-evals-lesson-8-improving-llm-products/</guid><description>&lt;p&gt;The final lesson of Hamel Husain and Shreya Shankar&amp;rsquo;s &lt;a href="https://thingsithinkithink.blog/posts/2025/06-08-llm-evals-lesson-1/"&gt;LLM evaluation course&lt;/a&gt; covered practical strategies for the &amp;ldquo;Improve&amp;rdquo; phase of the evaluation lifecycle. This post focuses on that lesson content in detail.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="improving-llm-based-systems"&gt;Improving LLM-Based Systems&lt;/h2&gt;
&lt;p&gt;With analysis and measurement complete, we can finally focus on improving our system. The lesson covered two areas: accuracy optimisation and cost reduction.&lt;/p&gt;
&lt;h3 id="improving-accuracy-quick-wins"&gt;Improving Accuracy: Quick Wins&lt;/h3&gt;
&lt;p&gt;Quick wins close the Gulf of Specification. If you&amp;rsquo;ve done error analysis properly, you&amp;rsquo;ll probably find a bunch of quick wins. You thought the system worked a certain way, you discovered it doesn&amp;rsquo;t, and often the fix is straightforward.&lt;/p&gt;</description></item><item><title>LLM Evals Course Lesson 7: Interfaces for Human Review</title><link>https://thingsithinkithink.blog/posts/2025/01-23-llm-evals-lesson-7-interfaces-for-human-review/</link><pubDate>Sun, 23 Nov 2025 12:00:00 +0000</pubDate><guid>https://thingsithinkithink.blog/posts/2025/01-23-llm-evals-lesson-7-interfaces-for-human-review/</guid><description>&lt;p&gt;The seventh lesson of Hamel Husain and Shreya Shankar&amp;rsquo;s &lt;a href="https://thingsithinkithink.blog/posts/2025/06-08-llm-evals-lesson-1/"&gt;LLM evaluation course&lt;/a&gt; covered interfaces for reviewing LLM outputs. This builds on &lt;a href="https://thingsithinkithink.blog/posts/2025/08-16-isaac-flath-fasthtml-annotation-tools/"&gt;Isaac Flath&amp;rsquo;s vibe coding sessions&lt;/a&gt;, but focuses on design principles rather than implementation.&lt;/p&gt;
&lt;h2 id="the-review-bottleneck-problem"&gt;The Review Bottleneck Problem&lt;/h2&gt;
&lt;p&gt;People talk a lot about AI doing more coding, pushing humans &amp;ldquo;higher up the stack&amp;rdquo; to do review instead of implementation. But review is already a massive bottleneck. I don&amp;rsquo;t want to manually examine 10,000 agent outputs one by one.&lt;/p&gt;</description></item><item><title>LLM Evals Course Lesson 6: Complex Pipelines and CI/CD</title><link>https://thingsithinkithink.blog/posts/2025/10-12-llm-evals-course-lesson-6-complex-pipelines-cicd/</link><pubDate>Sun, 12 Oct 2025 09:00:00 +0000</pubDate><guid>https://thingsithinkithink.blog/posts/2025/10-12-llm-evals-course-lesson-6-complex-pipelines-cicd/</guid><description>&lt;p&gt;The sixth lesson of Hamel Husain and Shreya Shankar&amp;rsquo;s &lt;a href="https://thingsithinkithink.blog/posts/2025/06-08-llm-evals-lesson-1/"&gt;LLM evaluation course&lt;/a&gt; wrapped up unfinished material from lesson 5 (evaluating complex architectures like RAG systems and tool calling) before moving into production concerns. We covered debugging agentic pipelines, handling complex data modalities, and implementing CI/CD for LLM applications.&lt;/p&gt;
&lt;h2 id="debugging-multi-step-and-agentic-pipelines"&gt;Debugging Multi-Step and Agentic Pipelines&lt;/h2&gt;
&lt;p&gt;Complex agentic applications have specific kinds of debugging challenges. You don&amp;rsquo;t know where to begin with error analysis when you have no idea what your agent is doing. It&amp;rsquo;s calling tools, invoking functions, making plans - and you need to figure out where to focus your efforts.&lt;/p&gt;</description></item><item><title>LLM Evals Course Lesson 5: How to Evaluate Complex Architectures</title><link>https://thingsithinkithink.blog/posts/2025/10-11-llm-evals-course-lesson-5-how-to-evaluate-complex-architectures/</link><pubDate>Sat, 11 Oct 2025 12:00:00 +0000</pubDate><guid>https://thingsithinkithink.blog/posts/2025/10-11-llm-evals-course-lesson-5-how-to-evaluate-complex-architectures/</guid><description>&lt;p&gt;The fifth lesson of Hamel Husain and Shreya Shankar&amp;rsquo;s &lt;a href="https://thingsithinkithink.blog/posts/2025/06-08-llm-evals-lesson-1/"&gt;LLM evaluation course&lt;/a&gt; moved into evaluating more complex systems. Previous lessons focused on single-step evaluation, but this session addressed RAG systems, tool calling, and other architectures that introduce additional components to evaluate.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;re still in the analyse-measure phase of the evaluation lifecycle. The focus remains on understanding issues and identifying failure modes rather than implementing improvements.&lt;/p&gt;
&lt;h2 id="rag-isnt-dead"&gt;RAG Isn&amp;rsquo;t Dead&lt;/h2&gt;
&lt;p&gt;The early post-ChatGPT period saw considerable attention on RAG implementation details - chunking strategies, vector databases, embedding models. Whilst these details matter, they became the single greatest focus when they&amp;rsquo;re just one part of a larger system.&lt;/p&gt;</description></item><item><title>LLM Evals Course Lesson 4: Multi-turn and Collaborative Evaluation</title><link>https://thingsithinkithink.blog/posts/2025/08-21-llm-evals-course-lesson-4-multiturn-collaborative-evaluation/</link><pubDate>Thu, 21 Aug 2025 12:00:00 +0000</pubDate><guid>https://thingsithinkithink.blog/posts/2025/08-21-llm-evals-course-lesson-4-multiturn-collaborative-evaluation/</guid><description>&lt;p&gt;The fourth lesson of Hamel Husain and Shreya Shankar&amp;rsquo;s &lt;a href="https://thingsithinkithink.blog/posts/2025/06-08-llm-evals-lesson-1/"&gt;LLM evaluation course&lt;/a&gt; covered two distinct challenges: evaluating multi-turn conversations and building evaluation criteria through collaboration.&lt;/p&gt;
&lt;h2 id="part-one-multi-turn-evaluation-beyond-single-exchanges"&gt;Part One: Multi-turn Evaluation Beyond Single Exchanges&lt;/h2&gt;
&lt;p&gt;Multi-turn evaluation presents unique challenges compared to single-turn interactions. The same analyse-measure-improve lifecycle applies, and binary criteria remain a good starting point. But conversations introduce new dimensions to consider.&lt;/p&gt;
&lt;h3 id="what-changes-with-multi-turn"&gt;What Changes with Multi-turn&lt;/h3&gt;
&lt;p&gt;When evaluating multi-turn conversations, three aspects come into play that don&amp;rsquo;t matter as much with individual turns:&lt;/p&gt;</description></item><item><title>LLM Evals Course Lesson 3: Building Automated Evaluators</title><link>https://thingsithinkithink.blog/posts/2025/07-06-llm-evals-course-lesson-3-automated-evaluators/</link><pubDate>Sun, 06 Jul 2025 08:00:00 +0000</pubDate><guid>https://thingsithinkithink.blog/posts/2025/07-06-llm-evals-course-lesson-3-automated-evaluators/</guid><description>&lt;p&gt;The third lesson of Hamel Husain and Shreya Shankar&amp;rsquo;s &lt;a href="https://thingsithinkithink.blog/posts/2025/06-08-llm-evals-lesson-1/"&gt;LLM evaluation course&lt;/a&gt; moved from manual error analysis into automated evaluation systems. This session focused on the &amp;ldquo;Measure&amp;rdquo; phase of their evaluation lifecycle - how to build evaluators that can automatically detect the failure modes we identified through error analysis.&lt;/p&gt;
&lt;p&gt;In terms of the Three Gulfs Model from lesson one, this lesson first helps us distinguish between specification failures (where we need to improve our prompts) and generalisation failures (where the LLM struggles despite clear instructions). The automated evaluators we build specifically target the Gulf of Generalisation, measuring how well our LLM applies instructions across diverse inputs.&lt;/p&gt;</description></item><item><title>LLM Evals Course: Lesson 2b (office hrs)</title><link>https://thingsithinkithink.blog/posts/2025/06-22-llm-evals-course-lesson-2b-office-hrs/</link><pubDate>Sun, 22 Jun 2025 09:00:00 +0000</pubDate><guid>https://thingsithinkithink.blog/posts/2025/06-22-llm-evals-course-lesson-2b-office-hrs/</guid><description>&lt;p&gt;A few things I wanted to note down from the first office hours session following &lt;a href="https://thingsithinkithink.blog/posts/2025/06-21-llm-evals-lesson-2-error-analysis/"&gt;lesson 2&lt;/a&gt; of Hamel and Shreya&amp;rsquo;s &lt;a href="https://thingsithinkithink.blog/posts/2025/06-08-llm-evals-lesson-1/"&gt;LLM evals course&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="getting-teams-to-actually-do-error-analysis"&gt;Getting Teams to Actually Do Error Analysis&lt;/h2&gt;
&lt;p&gt;Someone asked how to actually get your team to engage with error analysis? It&amp;rsquo;s one thing to say &amp;ldquo;look at your data,&amp;rdquo; but quite another to inspire people to do the work.&lt;/p&gt;
&lt;p&gt;Shreya&amp;rsquo;s answer was straightforward: run training sessions. There&amp;rsquo;s significant activation energy required to get people over the hump, but once they experience error analysis firsthand, they become self-fuelling. The process is so valuable that people understand its worth immediately after doing it properly once.&lt;/p&gt;</description></item><item><title>LLM Evals Lesson 2 Error Analysis</title><link>https://thingsithinkithink.blog/posts/2025/06-21-llm-evals-lesson-2-error-analysis/</link><pubDate>Sat, 21 Jun 2025 12:50:38 +0000</pubDate><guid>https://thingsithinkithink.blog/posts/2025/06-21-llm-evals-lesson-2-error-analysis/</guid><description>&lt;p&gt;The second lesson of Hamel Husain and Shreya Shankar&amp;rsquo;s LLM evaluation course tackled the &amp;ldquo;Analyse&amp;rdquo; phase of their evaluation lifecycle. This session focused on systematic error analysis - moving beyond gut feelings and random fixes to understand precisely where and why LLM applications fail.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://thingsithinkithink.blog/images/2025/parlancecourse/05_21_analyse_measure_improve.png" alt="The evaluation lifecycle showing Analyze, Measure, and Improve phases"&gt;&lt;/p&gt;
&lt;p&gt;If you&amp;rsquo;re going to make something better, you need to understand how it fails and error analysis focuses on that.&lt;/p&gt;</description></item><item><title>Hamel &amp; Shreya's LLM Evals Course: Lesson 1</title><link>https://thingsithinkithink.blog/posts/2025/06-08-llm-evals-lesson-1/</link><pubDate>Sun, 08 Jun 2025 12:00:00 +0000</pubDate><guid>https://thingsithinkithink.blog/posts/2025/06-08-llm-evals-lesson-1/</guid><description>&lt;p&gt;I&amp;rsquo;ve started taking Hamel Husain and Shreya Shankar&amp;rsquo;s course on evaluating LLM applications. The course attracted over 700 students from more than 300 companies, which gives you a sense of how much demand there is for systematic approaches to improving AI-driven products. As someone who has taught classes with a maximum of 120 students, I&amp;rsquo;m glad it&amp;rsquo;s not me having to monitor the lesson chat.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m writing these notes here for myself as a way to come back and check what I learned from the course.&lt;/p&gt;</description></item></channel></rss>