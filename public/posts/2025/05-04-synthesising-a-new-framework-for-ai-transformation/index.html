<!DOCTYPE html>
<html lang="en-us" dir="ltr">
<head>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>Synthesising a new framework for AI Transformation | thingsithinkithink</title>

<meta name="description" content="I like bits of Brunig&#39;s and Mollick&#39;s AI frameworks, but neither quite works for me.">
      <link rel="stylesheet" href="/css/main.min.1188bca307f4eccd3ab4edf9c00e4545a2d7f5d0c3f98a288950746b92c49d73.css" integrity="sha256-EYi8owf07M06tO35wA5FRaLX9dDD&#43;YooiVB0a5LEnXM=" crossorigin="anonymous">

<link rel="icon" type="image/svg+xml" href="https://thingsithinkithink.blog/favicon.svg"> 
<link rel="icon" type="image/x-icon" href="https://thingsithinkithink.blog/favicon.ico"> 
<link rel="icon" type="image/png" sizes="16x16" href="https://thingsithinkithink.blog/favicon.png"> 
<link rel="icon" type="image/png" sizes="32x32" href="https://thingsithinkithink.blog/favicon-32.png"> 
<link rel="icon" type="image/png" sizes="64x64" href="https://thingsithinkithink.blog/favicon-64.png"> 

</head>
<body>
  <header class="py-6 border-b"><div class="max-w-7xl mx-auto px-4 md:px-6 lg:px-8 flex flex-col">
	<div class="flex items-center">
		<div class="flex items-center">
			<button class="flex items-center space-x-2 rounded-full border py-1 pr-[5px] pl-3 group bg-zinc-100 hover:bg-zinc-200 toggle-button" data-target="menu-bar">
				<svg width="22" height="22" fill="currentColor" viewBox="0 0 16 16">
					<path fill-rule="evenodd" d="M2.5 12a.5.5 0 0 1 .5-.5h10a.5.5 0 0 1 0 1H3a.5.5 0 0 1-.5-.5zm0-4a.5.5 0 0 1 .5-.5h10a.5.5 0 0 1 0 1H3a.5.5 0 0 1-.5-.5zm0-4a.5.5 0 0 1 .5-.5h10a.5.5 0 0 1 0 1H3a.5.5 0 0 1-.5-.5z"></path>
				</svg>
				<span class="bg-blue-500 fill-white rounded-full p-1.5">
					<svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" fill="currentColor" class="group-hover:animate-spin bi bi-egg-fried fill-white" viewBox="0 0 16 16">
						<path d="M8 11a3 3 0 1 0 0-6 3 3 0 0 0 0 6z"></path>
						<path d="M13.997 5.17a5 5 0 0 0-8.101-4.09A5 5 0 0 0 1.28 9.342a5 5 0 0 0 8.336 5.109 3.5 3.5 0 0 0 5.201-4.065 3.001 3.001 0 0 0-.822-5.216zm-1-.034a1 1 0 0 0 .668.977 2.001 2.001 0 0 1 .547 3.478 1 1 0 0 0-.341 1.113 2.5 2.5 0 0 1-3.715 2.905 1 1 0 0 0-1.262.152 4 4 0 0 1-6.67-4.087 1 1 0 0 0-.2-1 4 4 0 0 1 3.693-6.61 1 1 0 0 0 .8-.2 4 4 0 0 1 6.48 3.273z"></path>
					</svg>
				</span>
			</button>
			<div class="relative rounded-full py-1.5 px-6 bg-zinc-100 hover:bg-zinc-200 text-xl font-bold uppercase mx-2">
				<h2><a class="before:content-[''] before:z-10 before:top-0 before:right-0 before:left-0 before:bottom-0 before:absolute before:pointer-events-auto" href="https://thingsithinkithink.blog/">thingsithinkithink</a></h2>
			</div>
		</div>
		<div class="flex items-center ml-auto">
			<button class="flex items-center rounded-full p-3 bg-zinc-100 hover:bg-zinc-200 toggle-button" data-target="search-bar">
				<svg width="18" height="18" fill="currentColor" viewBox="0 0 16 16">
					<path id="path1" class="transition-all ease-linear" d="M11.742 10.344a6.5 6.5 0 1 0-1.397 1.398h-.001c.03.04.062.078.098.115l3.85 3.85a1 1 0 0 0 1.415-1.414l-3.85-3.85a1.007 1.007 0 0 0-.115-.1zM12 6.5a5.5 5.5 0 1 1-11 0 5.5 5.5 0 0 1 11 0z"></path>
					<path id="path2" class="transition-all ease-linear hidden" d="M2.146 2.854a.5.5 0 1 1 .708-.708L8 7.293l5.146-5.147a.5.5 0 0 1 .708.708L8.707 8l5.147 5.146a.5.5 0 0 1-.708.708L8 8.707l-5.146 5.147a.5.5 0 0 1-.708-.708L7.293 8 2.146 2.854Z"></path>
				</svg>
			</button>
		</div>
	</div>
  <nav id="menu-bar" class="block mt-3 close">
    <ul class="flex items-center flex flex-nowrap whitespace-nowrap overflow-x-auto space-x-4">
    <li class="my-2">
      <a class="rounded-full border px-6 py-2 bg-zinc-100 hover:bg-zinc-200" href="/">Home</a>
    </li>
    <li class="my-2">
      <a class="rounded-full border px-6 py-2 bg-zinc-100 hover:bg-zinc-200" href="/about/">About</a>
    </li>
    <li class="my-2">
      <a class="rounded-full border px-6 py-2 bg-zinc-100 hover:bg-zinc-200" href="/tags/">Tags</a>
    </li>
    <li class="my-2">
      <a class="rounded-full border px-6 py-2 bg-zinc-100 hover:bg-zinc-200" href="/categories/">Taxonomy</a>
    </li>
    </ul>
  </nav>
<div id="search-bar" class="block mt-3 close">
	<form id="search" class="flex items-stretch">
		<input class="w-full block px-6 py-2 rounded-l-full focus:outline-none border border-zinc-200" type="text" placeholder="Search...">
		<button class="flex items-center px-7 py-2.5 rounded-r-full border border-zinc-200 ">
			<svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" fill="currentColor" class="group-hover:animate-pulse" viewBox="0 0 16 16">
				<path d="M11.742 10.344a6.5 6.5 0 1 0-1.397 1.398h-.001c.03.04.062.078.098.115l3.85 3.85a1 1 0 0 0 1.415-1.414l-3.85-3.85a1.007 1.007 0 0 0-.115-.1zM12 6.5a5.5 5.5 0 1 1-11 0 5.5 5.5 0 0 1 11 0z"></path>
			</svg>
		</button>
	</form>
</div>
</div></header>
  <main class="max-w-7xl mx-auto px-4 md:px-6 lg:px-8"><div id="breadcrumb" class="max-w-7xl mx-auto py-8">
	<ul class="flex space-x-4 text-sm text-zinc-500">
		<li class="after:content-['❯'] after:ml-4 after:opacity-30 last:after:content-none uppercase">
			<a href="https://thingsithinkithink.blog/">thingsithinkithink</a>
		</li>
		<li class="after:content-['❯'] after:ml-4 after:opacity-30 last:after:content-none uppercase">
			<a href="https://thingsithinkithink.blog/posts/">Posts</a>
		</li>
	</ul>
</div><div class="grid grid-cols-1 md:grid-cols-3 gap-6 mb-14">

  <article class="md:col-span-2 prose lg:prose-lg">

    <header class="not-prose">
      
      <h1 id="title" class="text-4xl font-bold leading-normal">Synthesising a new framework for AI Transformation</h1>

      <div id="lead" class="my-6">

        <p class="font-bold">I like bits of Brunig&#39;s and Mollick&#39;s AI frameworks, but neither quite works for me. </p>

      </div>
      
      <div id="writer" class="flex items-center space-x-4">

        <img class="w-12 h-12 bg-black rounded-full" src="https://thingsithinkithink.blog/images/goldfinch.jpg" alt="David Gérouville-Farrell avatar" width="1000" height="1000">

        <ul class="flex items-center space-x-4 flex-nowrap whitespace-nowrap overflow-x-auto">
          
          <li class="font-semibold my-2">David Gérouville-Farrell</li>
          
          <li class="before:content-['•'] before:mr-2 before:opacity-50 my-2"><time datetime="2025-05-04T12:00:00&#43;01:00">May 4, 2025</time>
          </li>

          <li class="before:content-['•'] before:mr-2 before:opacity-50 my-2">
            10 min read
          </li>

        </ul>

      </div>
      
    </header>

    <figure id="featureimage" class="rounded-xl aspect-video">
            <img class="rounded-lg" src="https://thingsithinkithink.blog/images/2025/05/mollick_on_transformation_hu_981a7cb03f05994e.png" alt="" width="750" height="294">
        <figcaption class="text-center italic text-xs">Mollick is right. We are all figureing this out as we go.</figcaption>

    </figure>

    <div id="content" class="mb-14">
      <p>Sparked by <a href="https://www.linkedin.com/posts/emollick_one-mistake-a-lot-of-companies-struggle-with-activity-7323721559481683969-j5Zm?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAAAKU3ZoBTHvzXgiOPqYTeQuTbQUqniMdk48">Ethan Mollick&rsquo;s post about AI transformation</a>, I&rsquo;ve been thinking about how we frame AI&rsquo;s role in workplace transformation. My current role involves doing this kind of work, and Ethan is correct to say we&rsquo;re all figuring it out at the same time. Therefore, in that light, here are some things I&rsquo;ve been thinking about lately.</p>
<p>I&rsquo;ve been considering two frameworks: <a href="https://www.dbreunig.com/2024/10/18/the-3-ai-use-cases-gods-interns-and-cogs.html">Drew Brunig&rsquo;s categorisation of AI use cases</a> as <strong>Cogs, Interns,</strong> and <strong>Gods</strong>, and <a href="https://www.oneusefulthing.org/p/on-boarding-your-ai-intern">Ethan Mollick&rsquo;s organisation of tasks</a> into <strong>Automated, Delegated,</strong> and <strong>Just-Me</strong> categories.</p>
<p>Both frameworks can be useful shorthands, but I find the &ldquo;Gods&rdquo; label unnecessarily sensational. In practice, we&rsquo;re dealing with tools, not deities. I prefer the metaphor/classification <strong>Colleagues</strong> for AI systems we trust to work independently, keeping our focus on the practical rather than the apocalyptic.</p>
<p>Likewise, in Ethan&rsquo;s framework, I feel &ldquo;Delegated&rdquo; doesn&rsquo;t evoke the human/AI dynamic that &ldquo;Intern&rdquo; does. So I&rsquo;ve been trying to find a way to bridge both frameworks to help us do this kind of work.</p>
<h2 id="the-core-mapping">The Core Mapping</h2>
<table>
  <thead>
      <tr>
          <th>Brunig (my edit)</th>
          <th>Mollick</th>
          <th>Key characteristic</th>
          <th>Human involvement</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Cogs</strong></td>
          <td>Automated tasks</td>
          <td>Focused single function</td>
          <td>Set it and forget it</td>
      </tr>
      <tr>
          <td><strong>Interns</strong></td>
          <td>Delegated tasks</td>
          <td>Grunt work supervised by responsible humans</td>
          <td>Review, corrections, key decision making</td>
      </tr>
      <tr>
          <td><strong>Colleagues</strong></td>
          <td>Just-Me tasks</td>
          <td>Complex judgement or human domains</td>
          <td>Full ownership (for now)</td>
      </tr>
  </tbody>
</table>
<p>I&rsquo;m going to work through this and try to show where I think we land when bringing these ideas together.</p>
<h2 id="cogs-the-automated-components">Cogs: The Automated Components</h2>
<p>In Brunig&rsquo;s framing, <strong>Cogs</strong> are &ldquo;designed to do one task, unsupervised, very well.&rdquo; They&rsquo;re comparable to functions that fit within larger systems, with each cog designed for &ldquo;one task and a low tolerance for errors.&rdquo;</p>
<p>These map cleanly to Mollick&rsquo;s <strong>Automated Tasks</strong> – &ldquo;ones you leave completely to the AI and don&rsquo;t even check on.&rdquo; Both describe focused, reliable components that perform specific functions without human intervention:</p>
<ul>
<li>Extracting postcodes from forms</li>
<li>Converting speech to text in meeting recordings</li>
<li>Classifying customer emails by urgency</li>
</ul>
<p>What makes these &ldquo;cog-like&rdquo; and automatable is their narrow focus and integration into larger processes. They run reliably enough that we don&rsquo;t need to watch them constantly, freeing up human attention for more complex work.</p>
<p>As Brunig notes, cogs are usually built with &ldquo;fine-tuned or heavily prompted small, open models.&rdquo; They&rsquo;re economical and purpose-built for reliability rather than versatility. However, the flexibility of large language models is leading to a &ldquo;big hammer, everything looks like a nail&rdquo; pattern. I&rsquo;ve already seen this happening in organisations that weren&rsquo;t previously AI-oriented - tasks like sentiment analysis or entity extraction that traditionally required specialized models are now being handled by general-purpose LLMs (albeit smaller ones) instead of custom-trained models. This doesn&rsquo;t refute Brunig&rsquo;s point, but suggests the tools used for cog-like tasks are evolving.</p>
<h2 id="interns-the-supervised-assistants">Interns: The Supervised Assistants</h2>
<p>The way I think about this intern level is that there&rsquo;s a relationship between a supervising human and an AI doing bits of work for them. When Ethan talks about jobs being comprised of bundles of tasks, it&rsquo;s these bundles that we delegate to the AI and then review. These bundles can include simple cogs but also more advanced tasks that are less reliable.</p>
<p>This is where I see today&rsquo;s &ldquo;agents&rdquo; fitting in. An agent isn&rsquo;t just a single cog - it&rsquo;s a collection of capabilities bundled together to accomplish broader goals. For example, a research agent might:</p>
<ul>
<li>Search and retrieve relevant documents</li>
<li>Extract key information from PDFs</li>
<li>Summarise findings in a consistent format</li>
<li>Draft responses based on the research</li>
</ul>
<p>Each component could theoretically be a separate cog, but wrapped together they form what we delegate as a bundle. The human remains responsible for reviewing the output and making key decisions, particularly when the agent encounters edge cases or ambiguity.</p>
<p>What&rsquo;s particularly valuable in thinking about bundles is that it helps us see the layers of AI workplace integration. Individual cogs can disappear into pipelines, while collections of cogs plus more advanced capabilities can be packaged as agents or delegated task systems. The human role shifts from performing each task to orchestrating and supervising these bundles.</p>
<p>As Mollick notes in his work, this seems to be where most productive human-AI collaboration currently happens - not with fully autonomous systems, but with responsible humans delegating grunt work while maintaining oversight and decision authority.</p>
<h2 id="from-gods-to-colleagues-a-reframing">From Gods to Colleagues: A Reframing</h2>
<p>As mentioned, I don&rsquo;t like the framing of highly competent AI as &ldquo;Gods&rdquo; - &ldquo;super-intelligent artificial entities that do things autonomously.&rdquo; I think <strong>Colleagues</strong> better captures what we&rsquo;re aiming for: systems that can work alongside us with minimal supervision while still operating within human systems and towards human-defined goals.</p>
<p>That said, an important reality check: despite all the lab announcements and AGI predictions, we&rsquo;re simply not there (yet). Those of us building systems on top of these AI models aren&rsquo;t likely to be interacting with truly autonomous human-level systems in the immediate future. The practical applications of AI in workplaces today don&rsquo;t include handing over full human-level jobs to AI systems.</p>
<p>Therefore, when using Brunig&rsquo;s framework of cogs, interns, and colleagues, my opinionated stance is that we can mostly ignore (for now) the idea of just handing over complete jobs to AIs. Instead, I find it more useful to focus on which parts of our work can be reconstituted as cogs or as interns (bundles of delegated tasks with human supervision).</p>
<h2 id="the-just-me-tasks-versus-colleagues-gods">The Just-Me Tasks versus Colleagues (Gods)&hellip;</h2>
<p>This is where Mollick&rsquo;s framework is particularly valuable. He addresses the most advanced layer differently by pointing out that there are tasks we just wouldn&rsquo;t ask an AI to do - he calls them &ldquo;Just-Me Tasks.&rdquo;</p>
<p>While Brunig&rsquo;s &ldquo;Gods&rdquo; category focuses on hypothetical future AI with fully autonomous capabilities, I find Mollick&rsquo;s approach more immediately practical and fundamentally more useful.</p>
<p>Since I believe the intern level (delegated tasks) is where most near-term value will come from, I think it&rsquo;s more useful to characterise the third layer in terms of what stays human rather than what becomes fully automated.</p>
<h3 id="1-beyond-the-jagged-frontier-temporary-just-me">1. Beyond the Jagged Frontier (Temporary Just-Me)</h3>
<p>These are tasks that remain human-only because <strong>current AI isn&rsquo;t good enough yet</strong>. They sit outside what Mollick calls the &ldquo;Jagged Frontier&rdquo; of AI capabilities.</p>
<ul>
<li>Complex strategic decision-making requiring deep contextual understanding</li>
<li>Novel research design drawing on multiple disciplines or lived experience</li>
<li>Nuanced client relationship management in highly variable situations</li>
</ul>
<p>The key point here is that some tasks are temporarily in the colleague/just-me layer because of capability reasons.</p>
<p>As AI improves, they won&rsquo;t suddenly become colleague-level tasks where AI works autonomously. Instead, they&rsquo;ll move into the intern category first - becoming part of bundles that require human supervision.</p>
<h3 id="2-principled-human-essential-tasks">2. Principled Human-Essential Tasks</h3>
<p>The second category involves tasks we keep human for <strong>ethical or fundamentally <em>human</em></strong> reasons – even if AI could perform them competently:</p>
<ul>
<li>Deciding on social support eligibility</li>
<li>Creating art for personal expression</li>
<li>Making final hiring decisions</li>
</ul>
<p>These tasks involve values, emotional connection, or ethical accountability that we believe require human ownership. Even if an AI could technically perform them, we choose to maintain human involvement as a matter of principle.</p>
<p>This distinction helps us think about which tasks will forever stay beyond the ownership of AI versus those which are currently beyond the capability of AI but will be brought in over time as models improve.</p>
<h2 id="what-work-looks-like-following-ai-automation">What Work Looks Like Following AI Automation</h2>
<p>Mollick makes an observation that helps tie these frameworks together: &ldquo;Jobs are composed of bundles of tasks. Jobs fit into larger systems.&rdquo;</p>
<p>A job title like &ldquo;analyst&rdquo; or &ldquo;designer&rdquo; is really a shorthand for a collection of responsibilities that exist within organisational contexts. The system aspects are crucial – a task doesn&rsquo;t exist in isolation but connects to processes, expectations, and relationships.</p>
<p>This systems perspective explains why technological change rarely eliminates entire professions outright. When spreadsheets automated calculations, they didn&rsquo;t eliminate accountants – they changed the bundle of tasks accountants perform, shifting focus from computation to analysis and strategy.</p>
<p>As AI takes over more tasks, our job descriptions shift from <strong>doing each task</strong> to <strong>ensuring the right outcome</strong>:</p>
<blockquote>
<p>I&rsquo;m no longer paid to write perfect documentation; I&rsquo;m paid to ensure users can successfully achieve their goals.</p></blockquote>
<p>This reframes seniority and expertise. Junior roles may still focus on reviewing AI outputs, while senior roles become what Mollick describes as &ldquo;orchestrators of pipelines,&rdquo; focusing on:</p>
<ol>
<li>Which task bundles can be safely automated</li>
<li>How to measure and improve the quality of AI outputs</li>
<li>When human intervention is necessary</li>
</ol>
<p>Organisations undergoing AI transformation will need to both redesign business processes into cogs, task bundles, and supervision points. But they will also have to undergo organisational redesign and job role redesign to remove bottlenecks and ensure smooth transformation.</p>
<h2 id="how-to-use-ai-in-transforming-work">How to Use AI in Transforming Work</h2>
<p>Trying to weave together these various threads into something coherent, here&rsquo;s where I&rsquo;m currently landing in terms of how to use AI in transforming work:</p>
<table>
  <thead>
      <tr>
          <th>Cogs (Automated Tasks)</th>
          <th>Intern Agents (Task Bundles)</th>
          <th>Human-Reserved Tasks</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Well-defined tasks that AI performs exceptionally well and can be trusted to do without supervision. These are single-purpose functions with low error rates that can be integrated directly into workflows. Might be agents or LLMs.  Might be Machine Learning</td>
          <td>Bundles of cogs and other tasks that AI Agents can significantly help with, but which require oversight from a responsible human. These represent collections of related activities where AI does the grunt work but humans make key decisions.</td>
          <td>Tasks unsuitable for AI involvement. There are two types of activities in this grouping: - (1) Tasks which AI is not currently very capable of doing - these will, over time, move into the Task Bundles layer - (2) Tasks that we decide, for ethical or <em>fundamentally human</em> reasons, should not be done by AI.</td>
      </tr>
  </tbody>
</table>
<p>Much of my current work involves helping organisations undergo AI transformation. This is how I&rsquo;m currently thinking through that process - identifying which activities fit into which category, and designing systems that maximize the value of cogs and interns while preserving human judgment where it matters most.</p>
<h2 id="thingsithinkithink">thingsithinkithink</h2>
<ul>
<li>
<p>The terminology shift from &ldquo;Gods&rdquo; to &ldquo;Colleagues&rdquo; isn&rsquo;t just semantic – it&rsquo;s grounded. We should be designing for partnership, not for magical genie-out-of-a-lamp replacement.</p>
</li>
<li>
<p>The distinction between skill-boundary and principled human-essential tasks clarifies many confused debates about &ldquo;jobs at risk.&rdquo; Some tasks are temporarily human; others are permanently so.</p>
</li>
<li>
<p>Jobs as systems of task bundles offers a much more nuanced view than the binary &ldquo;will AI replace X profession?&rdquo; headlines. The question isn&rsquo;t whether a job will vanish, but how its component tasks will shift across the jagged frontier, and what the job looks like following that transition.</p>
<ul>
<li>The above point doesn&rsquo;t ease the pain of those fields where work has been significantly negatively impacted by abundant access to AI. My friends who are visual artists, graphic designers, and writers are certainly feeling that impact.</li>
</ul>
</li>
<li>
<p>Regarding my dismissal of &ldquo;Gods&rdquo; - I&rsquo;m not saying AGI will never happen. But there&rsquo;s a diffusion challenge - how long will it take for the first AGI to be rolled out to every business that needs transformation? That&rsquo;s why I don&rsquo;t think those of us building practical systems will be building on true AGI over the next five years. There might be instances here and there, but most of us will be building complex, semi-autonomous, intern-level agents supervised by people with broad contextual understanding of their work environment.</p>
<ul>
<li>There&rsquo;s an absolute boatload of context that human beings have that we have no mechanism to transpose into a simple stream of tokens for even the most advanced AI systems. This isn&rsquo;t a claim about the timeline of AGI - it&rsquo;s about when us &ldquo;normies&rdquo; will actually be using these models in everyday business contexts.</li>
</ul>
</li>
</ul>


    <ul id="taxonomy" class="not-prose flex items-center space-x-4 flex-nowrap whitespace-nowrap overflow-x-auto">

      <li class="font-semibold my-4">Tags:</li>
        <li ><a class="py-2 px-6 border rounded-full hover:bg-zinc-100 active:bg-zinc-300" href="/tags/ai/">AI</a></li>
        <li ><a class="py-2 px-6 border rounded-full hover:bg-zinc-100 active:bg-zinc-300" href="/tags/transformation/">Transformation</a></li>
        <li ><a class="py-2 px-6 border rounded-full hover:bg-zinc-100 active:bg-zinc-300" href="/tags/framework/">Framework</a></li>
    </ul>
</div>

    <footer id="content-footer" class="not-prose">

      

      
      <div id="related-post" class="">

        <h2 class="text-xl md:text-2xl font-bold mb-6 md:mb-8">Recommended for You</h2><div class="grid grid-cols-1 md:grid-cols-2 gap-6"><article class="relative group bg-zinc-100 hover:bg-blue-100 rounded-3xl">
	
	<a class="insert-link" href="https://thingsithinkithink.blog/posts/2025/04-13-why-we-need-experiment-based-roadmaps-in-the-ai-product-era/"></a>

	<figure class="w-full aspect-video overflow-hidden rounded-3xl"><img class="object-cover group-hover:scale-105 transition duration-500 cursor-pointer" src="https://thingsithinkithink.blog/images/2025/hamel/14_hu_c5aba18e16637e87.png" alt="Why we need Experiment-based Roadmaps in the AI Product Era" width="750" height="336">

	  </figure>

	<div class="p-6">

		<time datetime="2025-04-12T00:00:00&#43;00:00">Apr 12, 2025</time>

		<h3 class="my-4 text-2xl font-bold">Why we need Experiment-based Roadmaps in the AI Product Era</h3>

		<p class="text-normal leading-normal text-zinc-500 line-clamp-2">Why evaluation-driven experimentation creates better roadmaps in AI products.</p>

	</div>
</article><article class="relative group bg-zinc-100 hover:bg-blue-100 rounded-3xl">
	
	<a class="insert-link" href="https://thingsithinkithink.blog/posts/2025/01-08-a-good-interview-with-simon-willison-on-around-the-prompt/"></a>

	<figure class="w-full aspect-video overflow-hidden rounded-3xl"><img class="object-cover group-hover:scale-105 transition duration-500 cursor-pointer" src="https://thingsithinkithink.blog/images/2025/01-08-a-good-interview-with-simon-willison-on-around-the-prompt_hu_b6aa693d8f92b30d.png" alt="The challenges of mastering LLMs, and their role as cyborg enhancement" width="750" height="354">

	  </figure>

	<div class="p-6">

		<time datetime="2025-01-08T12:00:00&#43;00:00">Jan 08, 2025</time>

		<h3 class="my-4 text-2xl font-bold">The challenges of mastering LLMs, and their role as cyborg enhancement</h3>

		<p class="text-normal leading-normal text-zinc-500 line-clamp-2">Simon Willison was a guest on Logan Kilpatrick&#39;s Google podcast. Topics covered: AI as a &#39;cyborg enhancement&#39;, the non-intuitive challenges of mastering LLM use, and the legitimate need for uncensored language models in fields like journalism.</p>

	</div>
</article>
          
        </div>
      </div>

    </footer>

  </article>

  
  <aside class="md:col-span-1"><div class="lg:sticky lg:top-8"><div class="rounded-2xl p-4 bg-zinc-100 mb-10">

		<img class="aspect-video rounded" src="/images/2024/infinite_archive.png" alt="things i think i think abstract image of waves and a stick figure">

		<p class="text-right text-xs mt-2 leading-none text-zinc-500">things i think i think</p>

</div><div class="space-y-6">

		<h2 class="font-bold text-xl mb-8">Recent Post</h2>

		<article class="relative group flex flex-row">
	<a class="insert-link" href="https://thingsithinkithink.blog/posts/2025/05-05-cogs-intern-agents-and-human-tasks--a-practical-framework-for-ai-transformation/"></a>
	<figure class="basis-1/3 aspect-square overflow-hidden rounded-2xl bg-zinc-100"><img class="object-cover h-full w-full group-hover:scale-105 transition duration-500 cursor-pointer" src="https://thingsithinkithink.blog/images/2025/05/cogs_interns_humans_hu_37b8c67dd07413d9.png" alt="Cogs / Interns / Human Tasks, a practical framework for AI transformation" width="250" height="250">

	</figure>
	<div class="basis-2/3 self-center ml-4">
		<h3 class="font-bold group-hover:underline decoration-auto">Cogs / Interns / Human Tasks, a practical framework for AI transformation</h3>
	</div>
</article><article class="relative group flex flex-row">
	<a class="insert-link" href="https://thingsithinkithink.blog/posts/2025/05-04-synthesising-a-new-framework-for-ai-transformation/"></a>
	<figure class="basis-1/3 aspect-square overflow-hidden rounded-2xl bg-zinc-100"><img class="object-cover h-full w-full group-hover:scale-105 transition duration-500 cursor-pointer" src="https://thingsithinkithink.blog/images/2025/05/mollick_on_transformation_hu_768e4f85300b0f29.png" alt="Synthesising a new framework for AI Transformation" width="250" height="250">

	</figure>
	<div class="basis-2/3 self-center ml-4">
		<h3 class="font-bold group-hover:underline decoration-auto">Synthesising a new framework for AI Transformation</h3>
	</div>
</article><article class="relative group flex flex-row">
	<a class="insert-link" href="https://thingsithinkithink.blog/posts/2025/04-13-error-analysis-for-improving-llm-applications/"></a>
	<figure class="basis-1/3 aspect-square overflow-hidden rounded-2xl bg-zinc-100"><img class="object-cover h-full w-full group-hover:scale-105 transition duration-500 cursor-pointer" src="https://thingsithinkithink.blog/images/2025/hamel/error_analysis/motivation_hu_ab8598fd057c9763.png" alt="Error Analysis for Improving LLM Applications" width="250" height="250">

	</figure>
	<div class="basis-2/3 self-center ml-4">
		<h3 class="font-bold group-hover:underline decoration-auto">Error Analysis for Improving LLM Applications</h3>
	</div>
</article><article class="relative group flex flex-row">
	<a class="insert-link" href="https://thingsithinkithink.blog/posts/2025/04-13-why-we-need-experiment-based-roadmaps-in-the-ai-product-era/"></a>
	<figure class="basis-1/3 aspect-square overflow-hidden rounded-2xl bg-zinc-100"><img class="object-cover h-full w-full group-hover:scale-105 transition duration-500 cursor-pointer" src="https://thingsithinkithink.blog/images/2025/hamel/14_hu_db7b55435fb32801.png" alt="Why we need Experiment-based Roadmaps in the AI Product Era" width="250" height="250">

	</figure>
	<div class="basis-2/3 self-center ml-4">
		<h3 class="font-bold group-hover:underline decoration-auto">Why we need Experiment-based Roadmaps in the AI Product Era</h3>
	</div>
</article><article class="relative group flex flex-row">
	<a class="insert-link" href="https://thingsithinkithink.blog/posts/2025/04-08-the-m-x-n-problem-in-software-architecture/"></a>
	<figure class="basis-1/3 aspect-square overflow-hidden rounded-2xl bg-zinc-100"><img class="object-cover h-full w-full group-hover:scale-105 transition duration-500 cursor-pointer" src="https://thingsithinkithink.blog/images/2025/mxn/IMG_1903_hu_9219414da33bde71.PNG" alt="The M×N Problem in Software Architecture" width="250" height="250">

	</figure>
	<div class="basis-2/3 self-center ml-4">
		<h3 class="font-bold group-hover:underline decoration-auto">The M×N Problem in Software Architecture</h3>
	</div>
</article>
		
	</div>

</div>
</aside>

</div>
  
</main>
  <footer class="bg-zinc-100 py-10 md:py-14"><div class="max-w-7xl mx-auto px-4 md:px-6 lg:px-8">

  <div class="flex flex-wrap space-y-6 mb-4">

    <div class="w-full md:w-3/5 flex flex-col space-y-4 md:pr-8 lg:pr-10">

      <a class="flex items-center group" href="https://thingsithinkithink.blog/">
        <svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" fill="currentColor" class="mr-2 group-hover:animate-spin" viewBox="0 0 16 16">
          <path d="M8 11a3 3 0 1 0 0-6 3 3 0 0 0 0 6z"/>
          <path d="M13.997 5.17a5 5 0 0 0-8.101-4.09A5 5 0 0 0 1.28 9.342a5 5 0 0 0 8.336 5.109 3.5 3.5 0 0 0 5.201-4.065 3.001 3.001 0 0 0-.822-5.216zm-1-.034a1 1 0 0 0 .668.977 2.001 2.001 0 0 1 .547 3.478 1 1 0 0 0-.341 1.113 2.5 2.5 0 0 1-3.715 2.905 1 1 0 0 0-1.262.152 4 4 0 0 1-6.67-4.087 1 1 0 0 0-.2-1 4 4 0 0 1 3.693-6.61 1 1 0 0 0 .8-.2 4 4 0 0 1 6.48 3.273z"/>
        </svg>
        
        <span class="text-2xl font-semibold uppercase">thingsithinkithink</span>
      </a>

      <p class="font-semibold">
        
      </p>

      

    </div>

    <div class="self-center flex flex-col w-full md:w-2/5">

<ul id="social-media" class="flex items-center space-x-4">
	
	
	<li> 
		<a class="w-12 h-12 rounded-full bg-white hover:bg-zinc-200 flex items-center justify-center p-2" 
		   href="https://linkedin.com/in/davidfarrell81" target="_blank" rel="noopener noreferrer">
			<img src="/images/linkedin-color-svgrepo-com.svg" alt="LinkedIn Logo" class="w-8 h-8">
		</a>
	</li>
	
	<li> 
		<a class="w-12 h-12 rounded-full bg-white hover:bg-zinc-200 flex items-center justify-center p-2" 
		   href="https://github.com/DavidFarrell/" target="_blank" rel="noopener noreferrer">
			<img src="/images/github-mark.svg" alt="GitHub Logo" class="w-8 h-8">
		</a>
	</li>
	<li> 
		<a class="w-12 h-12 rounded-full bg-white hover:bg-zinc-200 flex items-center justify-center p-2" 
		   href="https://bsky.app/profile/dgerouvillefarrell.bsky.social" target="_blank" rel="noopener noreferrer">
			<img src="/images/Bluesky_Logo.svg" alt="Bluesky Logo" class="w-8 h-8">
		</a>
	</li>
	
	
</ul>

    </div>
    
  </div>

  <div class="my-8">
    <ul class="flex items-center space-x-4">
      
        <li><a class="decoration-auto hover:underline font-semibold" href="/">Home</a></li>
      
        <li><a class="decoration-auto hover:underline font-semibold" href="/about/">About</a></li>
      
        <li><a class="decoration-auto hover:underline font-semibold" href="/tags/">Tags</a></li>
      
        <li><a class="decoration-auto hover:underline font-semibold" href="/categories/">Taxonomy</a></li>
      
    </ul>
  </div>

  <div class="border-t pt-4">

    <p class="text-sm">
      Powered by 
      <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a>, 
      <a href="https://pages.github.com/" target="_blank" rel="noopener">GitHub Pages</a> 
      and based on the 
      <a href="https://pehtheme-hugo.netlify.app/" target="_blank" rel="noopener">pehtheme</a> theme.
    </p>

  </div>
  
</div>
</footer>
      <script defer src="/js/insertoggle.474f9b0e08021c6519cff4e46df14ccf148285b2d3a23d6321d6e10f25c291fb.js" integrity="sha256-R0&#43;bDggCHGUZz/TkbfFMzxSChbLToj1jIdbhDyXCkfs=" crossorigin="anonymous"></script>
</body>
</html>