---
title: "Podcast interview with Prof Alan Brown about uses of AI in Government"
date: 2024-12-03T12:00:00Z
slug: /12-05-podcast-interview-with-prof-alan-brown-about-uses-of-ai-in-government/
description: "descriptiongoeshere"
image: images/placeholder.jpg
caption: Photo by Author on Unsplash
categories:
  - uncategorized
tags:
  - untagged
draft: false
---

I've been thinking recently about how to frame the ways that generative AI is going to make its way into enterprise and government.

A recent podcast interview between **Professor Alan Brown** and **Think Tank** touched upon this. Alan spent nearly two decades in the USA driving large-scale software programmes and leading R&D teams. After roles at **Carnegie Mellon University** and as an **IBM Distinguished Engineer**, he's now focused on digital transformation. In addition to his research at **Exeter**, he's recently published [_"Surviving and Thriving in the Age of AI"_](https://surviveaibook.com/).

He proposed three ways AI is being integrated into government:

---

## Large-Scale Formal Projects

The first involves large-scale projects requiring significant resourcesâ€”think facial recognition at borders or fraud detection across tax returns. These could be small 6 month projects or large investments, spanning three to four years, with formal requirements and extensive governance, rollout plans, change champions, etc. 

This is the 'devil we know' - yes, things are a wee bit different perhaps in the context of generative AI but we have a lot of knowledge about managing large-scale IT projects, much of which transfers to this new context.

---

## Informal Use and Shadow AI

The second way is through informal use, where staff might run queries through **ChatGPT** to get answers - for a whole variety of contexts and use cases and entirely 'off book'. This raises important questions, particularly around issues of data governance and accuracy. Hallucinations (where the system produces inaccurate information) might not happen often, but the consequences can be serious when they do. 

Government is by default risk averse (rightfully so) but this downside should be considered against what currently happens.

Are we confident that staff are always reading complex case notes in detail? What is worse, a person _not_ engaging with the material, or the material possibly containing a hallucination? I don't think the answers are easy here.

The default stance of risk averse organisations may be to discourage such use of AI. However, informal use happens regardless of oversight, bringing a pressure to focus on risk mitigation rather than outright prohibition. I'm reminded of "desire paths" where people ignore the designed paved route in favour of cutting across the grass.

---

## Embedded AI in Commercial Products

The third way is perhaps the most subtle: AI integration into commercial products. From email autocomplete to meeting summaries, to the "summarise this email" button that has appeared in my **Gmail**, AI is becoming embedded in our everyday tools (whether or not we asked for it). This integration makes it increasingly difficult to control or implement policies against AI use. 
